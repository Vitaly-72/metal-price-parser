import json
import requests
from bs4 import BeautifulSoup
import re
from datetime import datetime
from typing import List, Dict
from config import URLS

class Parser:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        }
    
    def parse_site(self, urls: List[str], site_type: str) -> List[Dict]:
        """–ü–∞—Ä—Å–∏–º —Å–∞–π—Ç"""
        all_products = []
        
        for url in urls:
            try:
                print(f"üîó –ü–∞—Ä—Å–∏–º: {url}")
                response = self.session.get(url, timeout=15)
                
                if response.status_code != 200:
                    print(f"‚ùå –û—à–∏–±–∫–∞ HTTP: {response.status_code}")
                    continue
                
                soup = BeautifulSoup(response.text, 'lxml')
                products = []
                
                if site_type == 'trimet':
                    products = self.parse_trimet(soup, url)
                elif site_type == 'parad':
                    products = self.parse_parad(soup, url)
                
                print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ —Ç–æ–≤–∞—Ä–æ–≤: {len(products)}")
                all_products.extend(products)
                
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
                continue
        
        return all_products
    
    def parse_trimet(self, soup, url: str) -> List[Dict]:
        """–ü–∞—Ä—Å–∏–º –¢—Ä–∏–º–µ—Ç"""
        products = []
        items = soup.find_all('div', class_='product-list__item-roznica')
        
        for item in items:
            try:
                name_elem = item.find('a', class_='data_name_product')
                price_elem = item.find('p', class_='price-type-roznica')
                
                if name_elem and price_elem:
                    name = name_elem.get_text(strip=True)
                    price_text = price_elem.get_text(strip=True)
                    
                    price_match = re.search(r'(\d+)\s*—Ä—É–±', price_text)
                    price = int(price_match.group(1)) if price_match else 0
                    
                    products.append({
                        'name': name,
                        'price': price,
                        'url': url,
                        'site': '–¢—Ä–∏–º–µ—Ç'
                    })
                    
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Ç–æ–≤–∞—Ä–∞ –¢—Ä–∏–º–µ—Ç: {e}")
                continue
        
        return products
    
    def parse_parad(self, soup, url: str) -> List[Dict]:
        """–ü–∞—Ä—Å–∏–º –ü–∞—Ä–∞–¥"""
        products = []
        items = soup.find_all('div', class_='product-thumb')
        
        for item in items:
            try:
                name_elem = item.find('h4')
                if name_elem:
                    name_elem = name_elem.find('a')
                price_elem = item.find('span', class_='price-new')
                
                if name_elem and price_elem:
                    name = name_elem.get_text(strip=True)
                    price_text = price_elem.get_text(strip=True)
                    
                    price_match = re.search(r'(\d+)', price_text.replace(' ', ''))
                    price = int(price_match.group(1)) if price_match else 0
                    
                    products.append({
                        'name': name,
                        'price': price,
                        'url': url,
                        'site': '–ü–∞—Ä–∞–¥'
                    })
                    
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Ç–æ–≤–∞—Ä–∞ –ü–∞—Ä–∞–¥: {e}")
                continue
        
        return products

def main():
    print("üöÄ –ó–∞–ø—É—Å–∫ —É–ø—Ä–æ—â–µ–Ω–Ω–æ–≥–æ –ø–∞—Ä—Å–µ—Ä–∞...")
    
    parser = Parser()
    results = {}
    
    for category, sites in URLS.items():
        print(f"\n{'='*50}")
        print(f"üì¶ –ö–ê–¢–ï–ì–û–†–ò–Ø: {category.upper()}")
        print(f"{'='*50}")
        
        category_results = {}
        
        # –ü–∞—Ä—Å–∏–º –¢—Ä–∏–º–µ—Ç
        if 'trimet' in sites:
            print("üîç –ü–∞—Ä—Å–∏–º –¢—Ä–∏–º–µ—Ç...")
            trimet_products = parser.parse_site(sites['trimet'], 'trimet')
            category_results['trimet'] = {
                'products': trimet_products,
                'count': len(trimet_products)
            }
        
        # –ü–∞—Ä—Å–∏–º –ü–∞—Ä–∞–¥
        if 'parad' in sites:
            print("üîç –ü–∞—Ä—Å–∏–º –ü–∞—Ä–∞–¥...")
            parad_products = parser.parse_site(sites['parad'], 'parad')
            category_results['parad'] = {
                'products': parad_products,
                'count': len(parad_products)
            }
        
        results[category] = category_results
    
    # –î–æ–±–∞–≤–ª—è–µ–º timestamp
    results['timestamp'] = datetime.now().isoformat()
    results['total_categories'] = len(URLS)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ñ–∞–π–ª
    with open('results.json', 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ results.json")
    
    # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    print(f"\nüìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
    print(f"{'='*50}")
    for category, data in results.items():
        if category not in ['timestamp', 'total_categories']:
            trimet_count = data.get('trimet', {}).get('count', 0)
            parad_count = data.get('parad', {}).get('count', 0)
            print(f"{category}: –¢—Ä–∏–º–µ—Ç={trimet_count}, –ü–∞—Ä–∞–¥={parad_count}")

if __name__ == "__main__":
    main()
